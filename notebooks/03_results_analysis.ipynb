{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis — Weather Transformer\n",
    "\n",
    "This notebook analyzes the trained Weather Transformer model:\n",
    "\n",
    "1. **Load trained model** and configuration\n",
    "2. **Compute evaluation metrics** (RMSE, MAE, ACC)\n",
    "3. **Compare with persistence baseline**\n",
    "4. **Visualize predictions and attention**\n",
    "5. **Discussion and future work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Project imports\n",
    "from src.models.weather_transformer import WeatherTransformer\n",
    "from src.data.dataset import WeatherBenchDataset, create_dataloaders\n",
    "from src.evaluation.metrics import (\n",
    "    compute_all_metrics,\n",
    "    persistence_baseline,\n",
    "    rmse,\n",
    "    mae,\n",
    "    anomaly_correlation_coefficient,\n",
    ")\n",
    "from src.visualization.plot_predictions import plot_prediction_comparison\n",
    "from src.visualization.plot_loss import plot_training_curves\n",
    "from src.visualization.plot_attention import plot_attention_maps\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CONFIG_PATH = \"../configs/default.yaml\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/best_model.pt\"\n",
    "DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Load config\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "model_cfg = config.get(\"model\", {})\n",
    "data_cfg = config.get(\"data\", {})\n",
    "\n",
    "# Create model\n",
    "model = WeatherTransformer(\n",
    "    in_channels=model_cfg.get(\"in_channels\", 4),\n",
    "    out_channels=model_cfg.get(\"out_channels\", 4),\n",
    "    img_height=data_cfg.get(\"lat_size\", 32),\n",
    "    img_width=data_cfg.get(\"lon_size\", 64),\n",
    "    patch_size=model_cfg.get(\"patch_size\", 4),\n",
    "    embed_dim=model_cfg.get(\"embed_dim\", 256),\n",
    "    num_heads=model_cfg.get(\"num_heads\", 8),\n",
    "    num_layers=model_cfg.get(\"num_layers\", 6),\n",
    "    mlp_ratio=model_cfg.get(\"mlp_ratio\", 4.0),\n",
    "    dropout=0.0,  # No dropout at inference\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "if Path(CHECKPOINT_PATH).exists():\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', '?')}\")\n",
    "else:\n",
    "    print(f\"No checkpoint found at {CHECKPOINT_PATH}\")\n",
    "    print(\"Using randomly initialized model for demo...\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel Parameters: {model.count_parameters():,}\")\n",
    "print(model.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "try:\n",
    "    test_dataset = WeatherBenchDataset(data_dir=DATA_DIR, split=\"test\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Test data not found. Creating synthetic test data for demo...\")\n",
    "    # Create synthetic data for demo\n",
    "    B, C, H, W = 100, 4, 32, 64\n",
    "    test_inputs = torch.randn(B, C, H, W)\n",
    "    test_targets = test_inputs + torch.randn(B, C, H, W) * 0.3\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_inputs, test_targets)\n",
    "\n",
    "# Create loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Run model on test data and collect predictions.\"\"\"\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    for inputs, targets in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        all_preds.append(predictions.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "        all_inputs.append(inputs.cpu())\n",
    "    \n",
    "    return (\n",
    "        torch.cat(all_inputs, dim=0),\n",
    "        torch.cat(all_preds, dim=0),\n",
    "        torch.cat(all_targets, dim=0),\n",
    "    )\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running model on test set...\")\n",
    "inputs, predictions, targets = evaluate(model, test_loader, device)\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Prediction shape: {predictions.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "VARIABLE_NAMES = [\"t850\", \"z500\", \"u10\", \"v10\"]\n",
    "\n",
    "# Model metrics\n",
    "model_metrics = compute_all_metrics(predictions, targets, VARIABLE_NAMES)\n",
    "\n",
    "# Persistence baseline metrics\n",
    "persist_metrics = persistence_baseline(inputs, targets)\n",
    "\n",
    "print(\"Model Metrics:\")\n",
    "for k, v in model_metrics.items():\n",
    "    print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "print(\"\\nPersistence Baseline:\")\n",
    "for k, v in persist_metrics.items():\n",
    "    print(f\"  {k}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Table\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAE\", \"ACC\"],\n",
    "    \"Model\": [\n",
    "        model_metrics[\"rmse\"],\n",
    "        model_metrics[\"mae\"],\n",
    "        model_metrics[\"acc\"],\n",
    "    ],\n",
    "    \"Persistence\": [\n",
    "        persist_metrics[\"rmse\"],\n",
    "        persist_metrics[\"mae\"],\n",
    "        persist_metrics[\"acc\"],\n",
    "    ],\n",
    "})\n",
    "\n",
    "results_df[\"Improvement\"] = (\n",
    "    (results_df[\"Persistence\"] - results_df[\"Model\"]) / results_df[\"Persistence\"] * 100\n",
    ")\n",
    "\n",
    "# For ACC, higher is better\n",
    "results_df.loc[results_df[\"Metric\"] == \"ACC\", \"Improvement\"] = (\n",
    "    (results_df.loc[results_df[\"Metric\"] == \"ACC\", \"Model\"] - \n",
    "     results_df.loc[results_df[\"Metric\"] == \"ACC\", \"Persistence\"]) /\n",
    "    abs(results_df.loc[results_df[\"Metric\"] == \"ACC\", \"Persistence\"]) * 100\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS COMPARISON TABLE\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df.style.format({\n",
    "    \"Model\": \"{:.6f}\",\n",
    "    \"Persistence\": \"{:.6f}\",\n",
    "    \"Improvement\": \"{:+.2f}%\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample to visualize\n",
    "sample_idx = 0\n",
    "\n",
    "sample_input = inputs[sample_idx].numpy()\n",
    "sample_pred = predictions[sample_idx].numpy()\n",
    "sample_target = targets[sample_idx].numpy()\n",
    "\n",
    "# Plot for each variable\n",
    "for var_idx, var_name in enumerate(VARIABLE_NAMES):\n",
    "    fig = plot_prediction_comparison(\n",
    "        sample_input, sample_pred, sample_target,\n",
    "        variable_idx=var_idx,\n",
    "        variable_names=VARIABLE_NAMES,\n",
    "        title=f\"6h Forecast — {var_name}\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"../checkpoints/training_log.json\"\n",
    "\n",
    "if Path(LOG_PATH).exists():\n",
    "    with open(LOG_PATH, \"r\") as f:\n",
    "        training_log = json.load(f)\n",
    "    \n",
    "    history = training_log.get(\"history\", {})\n",
    "    \n",
    "    fig = plot_training_curves(history)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nBest epoch: {training_log.get('best_epoch')}\")\n",
    "    print(f\"Best val loss: {training_log.get('best_val_loss'):.6f}\")\n",
    "else:\n",
    "    print(f\"Training log not found at {LOG_PATH}\")\n",
    "    print(\"Run training to generate training curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights from model\n",
    "sample_batch = inputs[:1].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attention_weights = model(sample_batch, return_attention=True)\n",
    "\n",
    "print(f\"Number of layers: {len(attention_weights)}\")\n",
    "print(f\"Attention shape per layer: {attention_weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention from different layers\n",
    "for layer_idx in [0, len(attention_weights) // 2, len(attention_weights) - 1]:\n",
    "    fig = plot_attention_maps(\n",
    "        attention_weights,\n",
    "        layer_idx=layer_idx,\n",
    "        head_idx=0,\n",
    "        n_patches_h=8,\n",
    "        n_patches_w=16,\n",
    "    )\n",
    "    plt.suptitle(f\"Attention — Layer {layer_idx}\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-variable RMSE comparison\n",
    "from src.evaluation.metrics import rmse_per_variable\n",
    "\n",
    "model_per_var = rmse_per_variable(predictions, targets).numpy()\n",
    "persist_per_var = rmse_per_variable(inputs, targets).numpy()\n",
    "\n",
    "x = np.arange(len(VARIABLE_NAMES))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, model_per_var, width, label=\"Model\", color=\"steelblue\")\n",
    "bars2 = ax.bar(x + width/2, persist_per_var, width, label=\"Persistence\", color=\"coral\")\n",
    "\n",
    "ax.set_xlabel(\"Variable\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_title(\"Per-Variable RMSE Comparison\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(VARIABLE_NAMES)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add improvement percentages\n",
    "for i, (m, p) in enumerate(zip(model_per_var, persist_per_var)):\n",
    "    improvement = (p - m) / p * 100\n",
    "    ax.annotate(\n",
    "        f\"{improvement:+.1f}%\",\n",
    "        xy=(i, min(m, p) - 0.05),\n",
    "        ha=\"center\", fontsize=10, fontweight=\"bold\",\n",
    "        color=\"green\" if improvement > 0 else \"red\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion\n",
    "\n",
    "### What Worked\n",
    "- **Transformer architecture** successfully learns spatial dependencies in weather data\n",
    "- **Patch embedding** efficiently tokenizes 2D weather grids\n",
    "- **Physics-informed loss** encourages physically realistic predictions\n",
    "- **Positional encoding** helps model learn spatial awareness\n",
    "\n",
    "### What Could Be Improved\n",
    "- **Multi-step forecasting**: Currently single-step (6h), extend to autoregressive rollouts\n",
    "- **Variable interactions**: Add cross-attention between different weather variables\n",
    "- **Higher resolution**: Scale to full ERA5 resolution (721×1440)\n",
    "- **Temporal encoding**: Add time-of-year embeddings for seasonality\n",
    "\n",
    "### Future Work\n",
    "1. Implement autoregressive multi-day forecasting\n",
    "2. Add ensemble predictions for uncertainty quantification\n",
    "3. Compare with GraphCast/FourCastNet architectures\n",
    "4. Train on full WeatherBench2 dataset\n",
    "5. Add extreme weather event analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    \"model\": model_metrics,\n",
    "    \"persistence\": persist_metrics,\n",
    "    \"config\": config,\n",
    "    \"model_params\": model.count_parameters(),\n",
    "}\n",
    "\n",
    "output_path = Path(\"../results/metrics.json\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: Weather Transformer ({model.count_parameters():,} params)\")\n",
    "print(f\"Test Samples: {len(inputs)}\")\n",
    "print(f\"\")\n",
    "print(f\"Key Results:\")\n",
    "print(f\"  RMSE: {model_metrics['rmse']:.4f} (Persistence: {persist_metrics['rmse']:.4f})\")\n",
    "print(f\"  MAE:  {model_metrics['mae']:.4f} (Persistence: {persist_metrics['mae']:.4f})\")\n",
    "print(f\"  ACC:  {model_metrics['acc']:.4f} (Persistence: {persist_metrics['acc']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
