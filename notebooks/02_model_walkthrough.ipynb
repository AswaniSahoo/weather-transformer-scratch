{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Walkthrough — Weather Transformer from Scratch\n",
    "\n",
    "This notebook walks through each component of the Weather Transformer, explaining the theory and testing each piece individually.\n",
    "\n",
    "## Contents\n",
    "1. [Patch Embedding](#1-patch-embedding)\n",
    "2. [Positional Encoding](#2-positional-encoding)\n",
    "3. [Multi-Head Self-Attention](#3-multi-head-self-attention)\n",
    "4. [Transformer Block](#4-transformer-block)\n",
    "5. [Full Weather Transformer](#5-full-weather-transformer)\n",
    "6. [Physics-Informed Loss](#6-physics-informed-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Test dimensions\n",
    "B = 2        # Batch size\n",
    "C = 4        # Channels (weather variables)\n",
    "H, W = 32, 64  # Spatial dimensions (lat, lon)\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Patch Embedding\n",
    "\n",
    "### Theory\n",
    "\n",
    "The Patch Embedding layer converts a 2D weather grid into a sequence of tokens that the transformer can process.\n",
    "\n",
    "**Steps:**\n",
    "1. Split the input `(B, C, H, W)` into non-overlapping patches of size `P×P`\n",
    "2. Flatten each patch and project to embedding dimension\n",
    "\n",
    "**Implementation:** We use `nn.Conv2d` with `kernel_size=patch_size` and `stride=patch_size` — this elegantly handles both the patch extraction AND linear projection in one operation.\n",
    "\n",
    "```\n",
    "Input:  (B, 4, 32, 64)  →  4 variables on 32×64 grid\n",
    "Output: (B, 128, 256)   →  128 patch tokens, each 256-dim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.patch_embedding import PatchEmbedding\n",
    "\n",
    "# Create patch embedding layer\n",
    "patch_embed = PatchEmbedding(\n",
    "    in_channels=C,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    img_height=H,\n",
    "    img_width=W,\n",
    ")\n",
    "\n",
    "# Test input\n",
    "x = torch.randn(B, C, H, W)\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "patches = patch_embed(x)\n",
    "print(f\"Output shape: {patches.shape}\")\n",
    "print(f\"Number of patches: {patch_embed.n_patches}\")\n",
    "print(f\"Patch grid: {patch_embed.n_patches_h} × {patch_embed.n_patches_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patch grid\n",
    "n_h, n_w = patch_embed.n_patches_h, patch_embed.n_patches_w\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original image with patch grid\n",
    "ax1.imshow(x[0, 0].numpy(), cmap='RdBu_r')\n",
    "for i in range(n_h + 1):\n",
    "    ax1.axhline(i * PATCH_SIZE - 0.5, color='white', linewidth=0.5)\n",
    "for j in range(n_w + 1):\n",
    "    ax1.axvline(j * PATCH_SIZE - 0.5, color='white', linewidth=0.5)\n",
    "ax1.set_title(f\"Input with {n_h}×{n_w} patch grid\")\n",
    "\n",
    "# Embedding visualization\n",
    "ax2.imshow(patches[0].detach().numpy().T, aspect='auto', cmap='viridis')\n",
    "ax2.set_xlabel(\"Patch Index\")\n",
    "ax2.set_ylabel(\"Embedding Dimension\")\n",
    "ax2.set_title(\"Patch Embeddings (256-dim per patch)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Positional Encoding\n",
    "\n",
    "### Theory\n",
    "\n",
    "Transformers have no inherent notion of position — they see a \"bag of tokens\". Positional encoding adds spatial information.\n",
    "\n",
    "**Two variants:**\n",
    "\n",
    "1. **Learnable** (like ViT): Each position gets a learned embedding vector\n",
    "2. **Sinusoidal** (like original Transformer): Fixed sin/cos patterns at different frequencies\n",
    "\n",
    "We use **2D sinusoidal encoding** that separately encodes latitude and longitude positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.positional_encoding import (\n",
    "    LearnablePositionalEncoding,\n",
    "    SinusoidalPositionalEncoding,\n",
    ")\n",
    "\n",
    "N_PATCHES = patch_embed.n_patches\n",
    "\n",
    "# Learnable\n",
    "learnable_pe = LearnablePositionalEncoding(\n",
    "    n_patches=N_PATCHES,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "# Sinusoidal\n",
    "sinusoidal_pe = SinusoidalPositionalEncoding(\n",
    "    n_patches=N_PATCHES,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    n_patches_h=n_h,\n",
    "    n_patches_w=n_w,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "# Apply both\n",
    "out_learn = learnable_pe(patches)\n",
    "out_sin = sinusoidal_pe(patches)\n",
    "\n",
    "print(f\"After positional encoding: {out_learn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sinusoidal positional encodings\n",
    "pe = sinusoidal_pe.position_embeddings[0].numpy()  # (N, D)\n",
    "pe_2d = pe.reshape(n_h, n_w, EMBED_DIM)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "fig.suptitle(\"2D Sinusoidal Positional Encoding — First 8 dimensions\", fontsize=12)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(pe_2d[:, :, i], cmap='RdBu_r')\n",
    "    ax.set_title(f\"Dim {i}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.colorbar(im, ax=ax, shrink=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Multi-Head Self-Attention\n",
    "\n",
    "### Theory\n",
    "\n",
    "Self-attention allows each token to \"look at\" every other token and aggregate information based on learned relevance.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K.T / sqrt(d_k)) @ V\n",
    "```\n",
    "\n",
    "**Multi-head:** We run `num_heads` parallel attention operations, each with a subset of the embedding dimensions, then concatenate.\n",
    "\n",
    "**Key insight:** We implement this from scratch — no `nn.MultiheadAttention`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.attention import MultiHeadSelfAttention\n",
    "\n",
    "# Create attention layer\n",
    "attention = MultiHeadSelfAttention(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "# Apply attention\n",
    "x_embedded = out_sin  # Use output from positional encoding\n",
    "attn_out, attn_weights = attention(x_embedded, return_attention=True)\n",
    "\n",
    "print(f\"Attention output: {attn_out.shape}\")\n",
    "print(f\"Attention weights: {attn_weights.shape}\")\n",
    "print(f\"  → (batch, num_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "attn = attn_weights[0].detach().numpy()  # (num_heads, N, N)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "fig.suptitle(\"Multi-Head Attention Patterns\", fontsize=12)\n",
    "\n",
    "for head_idx, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(attn[head_idx], cmap='hot', aspect='auto')\n",
    "    ax.set_title(f\"Head {head_idx}\")\n",
    "    ax.set_xlabel(\"Key\")\n",
    "    ax.set_ylabel(\"Query\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what a specific patch attends to\n",
    "query_patch = N_PATCHES // 2  # Center patch\n",
    "query_h, query_w = query_patch // n_w, query_patch % n_w\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "fig.suptitle(f\"What does patch ({query_h}, {query_w}) attend to?\", fontsize=12)\n",
    "\n",
    "for head_idx, ax in enumerate(axes.flat):\n",
    "    attn_map = attn[head_idx, query_patch].reshape(n_h, n_w)\n",
    "    im = ax.imshow(attn_map, cmap='hot')\n",
    "    ax.scatter([query_w], [query_h], c='cyan', s=100, marker='*')\n",
    "    ax.set_title(f\"Head {head_idx}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transformer Block\n",
    "\n",
    "### Theory\n",
    "\n",
    "A transformer block combines:\n",
    "1. **Layer Norm** → **Multi-Head Attention** → **Residual**\n",
    "2. **Layer Norm** → **MLP (Feed-Forward)** → **Residual**\n",
    "\n",
    "We use **Pre-Norm** architecture (LayerNorm before each sublayer) — more stable training.\n",
    "\n",
    "```\n",
    "x → LayerNorm → MHSA → + → LayerNorm → MLP → + → output\n",
    "     └─────────────────┘    └───────────────┘\n",
    "          residual              residual\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.transformer_block import TransformerBlock\n",
    "\n",
    "# Create transformer block\n",
    "block = TransformerBlock(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "# Apply block\n",
    "block_out, block_attn = block(x_embedded, return_attention=True)\n",
    "\n",
    "print(f\"Block output: {block_out.shape}\")\n",
    "print(f\"Residual connection preserved shape ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the information flow — how much does the output differ from input?\n",
    "diff = (block_out - x_embedded).abs().mean(dim=-1)[0].detach().numpy()  # (N,)\n",
    "diff_2d = diff.reshape(n_h, n_w)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.imshow(diff_2d, cmap='viridis')\n",
    "ax1.set_title(\"Change magnitude per patch\")\n",
    "ax1.set_xlabel(\"Patch W\")\n",
    "ax1.set_ylabel(\"Patch H\")\n",
    "\n",
    "ax2.hist(diff.flatten(), bins=30, edgecolor='black')\n",
    "ax2.set_title(\"Distribution of changes\")\n",
    "ax2.set_xlabel(\"Absolute change\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Full Weather Transformer\n",
    "\n",
    "### Theory\n",
    "\n",
    "Now we stack everything together:\n",
    "\n",
    "1. **Patch Embedding** — Grid to tokens\n",
    "2. **Positional Encoding** — Add spatial info\n",
    "3. **N × Transformer Blocks** — Learn cross-patch relationships\n",
    "4. **Prediction Head** — Tokens back to grid\n",
    "\n",
    "The model predicts the **next weather state** (t+6h) given the current state (t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.weather_transformer import WeatherTransformer\n",
    "\n",
    "# Create full model\n",
    "model = WeatherTransformer(\n",
    "    in_channels=C,\n",
    "    out_channels=C,\n",
    "    img_height=H,\n",
    "    img_width=W,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=6,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for k, v in model.get_config().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = torch.randn(B, C, H, W)\n",
    "output, all_attentions = model(x, return_attention=True)\n",
    "\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention weights from {len(all_attentions)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction\n",
    "var_idx = 0  # First variable\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(x[0, var_idx].numpy(), cmap='RdBu_r')\n",
    "axes[0].set_title(\"Input (t)\")\n",
    "\n",
    "axes[1].imshow(output[0, var_idx].detach().numpy(), cmap='RdBu_r')\n",
    "axes[1].set_title(\"Prediction (t+6h)\")\n",
    "\n",
    "diff = (output[0, var_idx] - x[0, var_idx]).detach().numpy()\n",
    "axes[2].imshow(diff, cmap='coolwarm')\n",
    "axes[2].set_title(\"Predicted Change\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Physics-Informed Loss\n",
    "\n",
    "### Theory\n",
    "\n",
    "Standard MSE loss only penalizes pixel-wise errors. We add physics-based constraints:\n",
    "\n",
    "**Total Loss = α×MSE + β×Smoothness + γ×Conservation**\n",
    "\n",
    "1. **MSE** — Standard reconstruction loss\n",
    "2. **Smoothness** — Penalizes unrealistic sharp gradients (weather fields should be smooth)\n",
    "3. **Conservation** — Global mean should not change drastically (energy conservation proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.physics_loss import (\n",
    "    PhysicsInformedLoss,\n",
    "    SpatialSmoothnessLoss,\n",
    "    ConservationLoss,\n",
    ")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = PhysicsInformedLoss(\n",
    "    mse_weight=1.0,\n",
    "    smoothness_weight=0.1,\n",
    "    conservation_weight=0.05,\n",
    ")\n",
    "\n",
    "# Compute loss\n",
    "target = x + torch.randn_like(x) * 0.3  # Synthetic target\n",
    "losses = loss_fn(output, target)\n",
    "\n",
    "print(\"Loss Components:\")\n",
    "for k, v in losses.items():\n",
    "    print(f\"  {k}: {v.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate smoothness loss behavior\n",
    "smooth = torch.ones(1, 4, 32, 64)  # Perfectly smooth\n",
    "noisy = torch.randn(1, 4, 32, 64)  # Random noise\n",
    "\n",
    "smooth_loss = SpatialSmoothnessLoss()\n",
    "print(f\"Smoothness loss (uniform field): {smooth_loss(smooth).item():.6f}\")\n",
    "print(f\"Smoothness loss (random noise):  {smooth_loss(noisy).item():.6f}\")\n",
    "print(\"→ Noisy inputs get penalized more ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate conservation loss behavior\n",
    "conservation_loss = ConservationLoss()\n",
    "\n",
    "pred_same_mean = torch.randn(1, 4, 32, 64)\n",
    "target_same_mean = pred_same_mean.clone()  # Same mean\n",
    "target_diff_mean = pred_same_mean + 5.0     # Different mean\n",
    "\n",
    "print(f\"Conservation loss (same mean):      {conservation_loss(pred_same_mean, target_same_mean).item():.6f}\")\n",
    "print(f\"Conservation loss (different mean): {conservation_loss(pred_same_mean, target_diff_mean).item():.6f}\")\n",
    "print(\"→ Mean-preserving predictions are preferred ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've built a complete Weather Transformer from scratch:\n",
    "\n",
    "| Component | Purpose | Key Insight |\n",
    "|-----------|---------|-------------|\n",
    "| **Patch Embedding** | Tokenize 2D grid | Conv2d with stride=patch_size |\n",
    "| **Positional Encoding** | Add spatial info | 2D sinusoidal for lat/lon |\n",
    "| **Multi-Head Attention** | Cross-patch relationships | Parallel attention heads |\n",
    "| **Transformer Block** | Process tokens | Pre-norm + residuals |\n",
    "| **Physics Loss** | Realistic predictions | Smoothness + conservation |\n",
    "\n",
    "**Next steps:**\n",
    "- Train on real ERA5 data\n",
    "- Evaluate against persistence baseline\n",
    "- Visualize learned attention patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
