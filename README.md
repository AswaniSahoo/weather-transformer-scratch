# ğŸŒ¦ï¸ Weather Transformer from Scratch

> A physics-aware Vision Transformer for weather forecasting, built entirely from scratch in PyTorch.

**Built as preparation for GSoC 2026 â€” AI for Science**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests](https://img.shields.io/badge/tests-74%20passed-brightgreen.svg)]()

---

## ğŸ¯ Project Overview

This project implements a **Vision Transformer (ViT)** architecture for weather forecasting, trained on ERA5 reanalysis data from WeatherBench2. Key features:

- âœ… **Every component built from scratch** â€” no `nn.MultiheadAttention`
- âœ… **Physics-informed loss** â€” MSE + spatial smoothness + conservation constraints
- âœ… **Real climate data** â€” WeatherBench2 / ERA5 at 5.625Â° resolution (2015â€“2020)
- âœ… **Comprehensive evaluation** â€” RMSE, MAE, ACC vs persistence baseline
- âœ… **Production-ready** â€” Config-driven training, checkpointing, logging
- âœ… **74 unit tests** â€” Full test coverage across data, model, and metrics

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Weather Transformer                       â”‚
â”‚                    4,805,440 parameters                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Input (B, 4, 32, 64)     4 weather variables, latÃ—lon     â”‚
â”‚          â”‚                                                   â”‚
â”‚          â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚   â”‚  Patch Embedding â”‚  Conv2d â†’ (B, 128, 256)              â”‚
â”‚   â”‚  (4Ã—4 patches)   â”‚  128 patches, 256-dim embeddings     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚            â”‚                                                 â”‚
â”‚            â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚   â”‚  + Positional    â”‚  Learnable spatial position info     â”‚
â”‚   â”‚    Encoding      â”‚                                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚            â”‚                                                 â”‚
â”‚            â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚   â”‚  Transformer     â”‚  Ã— 6 layers                          â”‚
â”‚   â”‚  Encoder Blocks  â”‚  - Multi-Head Self-Attention (8h)    â”‚
â”‚   â”‚                  â”‚  - Feed-Forward MLP (4Ã— expansion)   â”‚
â”‚   â”‚                  â”‚  - Pre-Norm + Residual connections   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚            â”‚                                                 â”‚
â”‚            â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚   â”‚  Prediction Head â”‚  Linear â†’ Reshape â†’ (B, 4, 32, 64)   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                              â”‚
â”‚   Output: Predicted weather state at t+6h                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Results

### Model vs Persistence Baseline (2020 Test Set)

| Metric | Model | Persistence | Improvement |
|--------|-------|-------------|-------------|
| **RMSE** | 0.197 | 0.270 | **27.0% âœ…** |
| **MAE** | 0.126 | 0.147 | **13.9% âœ…** |
| **ACC** | 0.955 | 0.912 | **+4.3 pts âœ…** |

> *Evaluated on 1,316 test samples from year 2020. Persistence baseline: predict Y(t+1) = X(t).*

### Per-Variable RMSE

| Variable | RMSE | Description |
|----------|------|-------------|
| t850 | **0.083** | Temperature at 850 hPa â€” *best predicted* |
| z500 | **0.067** | Geopotential at 500 hPa â€” *smoothest field* |
| u10 | **0.242** | U-wind at 10m â€” *more chaotic* |
| v10 | **0.292** | V-wind at 10m â€” *hardest to predict* |

> Wind components (u10, v10) have higher RMSE because wind fields are inherently more turbulent and less spatially smooth than temperature and geopotential fields.

### Training Details

| Property | Value |
|----------|-------|
| Parameters | 4,805,440 |
| Training samples | 6,136 (2015â€“2018) |
| Validation samples | 1,315 (2019) |
| Test samples | 1,316 (2020) |
| Epochs | 50 |
| Training time | ~12 min (NVIDIA GPU) |
| Best epoch | 49 |
| Best val loss | 0.0699 |
| Convergence | Smooth = every epoch improved â­ |

### Sample Prediction

![Prediction Sample](results/figures/prediction_sample.png)

*6-hour forecast for temperature at 850 hPa. Left: Input, Middle: Prediction, Right: Ground Truth*

---

## ğŸ“ Project Structure

```
weather-transformer-scratch/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Hyperparameters & paths
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download.py           # WeatherBench2 data download
â”‚   â”‚   â”œâ”€â”€ preprocessing.py      # Preprocessing & normalization
â”‚   â”‚   â””â”€â”€ dataset.py            # PyTorch Dataset & DataLoader
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ patch_embedding.py    # Image â†’ patch tokens
â”‚   â”‚   â”œâ”€â”€ positional_encoding.py # Learnable + Sinusoidal PE
â”‚   â”‚   â”œâ”€â”€ attention.py          # Multi-head self-attention (from scratch)
â”‚   â”‚   â”œâ”€â”€ transformer_block.py  # Attention + MLP + residual + pre-norm
â”‚   â”‚   â”œâ”€â”€ weather_transformer.py # Full model assembly
â”‚   â”‚   â””â”€â”€ physics_loss.py       # MSE + smoothness + conservation loss
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py            # Training loop with checkpointing
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py            # RMSE, MAE, ACC, persistence baseline
â”‚   â”‚   â””â”€â”€ evaluate.py           # Evaluation script
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ plot_predictions.py   # World map predictions with cartopy
â”‚       â”œâ”€â”€ plot_loss.py          # Training/validation loss curves
â”‚       â””â”€â”€ plot_attention.py     # Attention weight visualization
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb # Data inspection & statistics
â”‚   â”œâ”€â”€ 02_model_walkthrough.ipynb # Step-by-step model building
â”‚   â””â”€â”€ 03_results_analysis.ipynb # Results & visualizations
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py                  # Training entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_dataset.py           # Dataset tests (10)
â”‚   â”œâ”€â”€ test_model.py             # Model tests (51)
â”‚   â””â”€â”€ test_metrics.py           # Metrics tests (13)
â”œâ”€â”€ checkpoints/                  # Saved model weights
â”œâ”€â”€ results/                      # Evaluation outputs & figures
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/AswaniSahoo/weather-transformer-scratch.git
cd weather-transformer-scratch

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# For GPU training (NVIDIA GPUs), install PyTorch with CUDA:
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### 2. Download Data

```bash
python src/data/download.py --start-year 2015 --end-year 2020
python src/data/preprocessing.py
```

### 3. Train Model

```bash
# CPU training
python scripts/train.py --config configs/default.yaml

# GPU training (recommended)
python scripts/train.py --config configs/default.yaml --device cuda

# GPU training with custom epochs
python scripts/train.py --config configs/default.yaml --device cuda --epochs 50
```

### 4. Evaluate

```bash
python -m src.evaluation.evaluate --checkpoint checkpoints/best_model.pt
```

### 5. Run Tests

```bash
pytest tests/ -v
```

---

## ğŸ”§ Configuration

All hyperparameters are defined in `configs/default.yaml`:

```yaml
model:
  in_channels: 4
  embed_dim: 256
  num_heads: 8
  num_layers: 6
  patch_size: 4
  mlp_ratio: 4.0
  dropout: 0.1

training:
  epochs: 50
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.01
  loss:
    mse_weight: 1.0
    smoothness_weight: 0.1
    conservation_weight: 0.05

data:
  variables: [t850, z500, u10, v10]
  lat_size: 32
  lon_size: 64
```

---

## ğŸ“ˆ Weather Variables

| Variable | Description | Level | Unit |
|----------|-------------|-------|------|
| `t850` | Temperature | 850 hPa | K |
| `z500` | Geopotential | 500 hPa | mÂ²/sÂ² |
| `u10` | U-wind component | 10m | m/s |
| `v10` | V-wind component | 10m | m/s |

---

## ğŸ§ª Testing

The project includes **74 comprehensive unit tests** across 3 test files:

| Test File | Tests | Coverage |
|-----------|-------|----------|
| `test_model.py` | 51 | Patch embedding, positional encoding, attention, transformer block, full model, physics loss |
| `test_dataset.py` | 10 | Data loading, tensor shapes, normalization, DataLoader |
| `test_metrics.py` | 13 | RMSE, MAE, ACC, persistence baseline |

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_model.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ“š Key Components

### Physics-Informed Loss

```python
L = Î± Ã— MSE + Î² Ã— Smoothness + Î³ Ã— Conservation

# MSE: Standard pixel-wise reconstruction
# Smoothness: Penalizes unrealistic spatial gradients (âˆ‚T/âˆ‚x, âˆ‚T/âˆ‚y)
# Conservation: Predicted global mean â‰ˆ target global mean (energy proxy)
```

### Multi-Head Self-Attention (from scratch)

```python
# No nn.MultiheadAttention â€” built entirely manually!
Q = x @ W_q  # Query projection
K = x @ W_k  # Key projection
V = x @ W_v  # Value projection

attention = softmax(Q @ K.T / sqrt(d_k)) @ V
```

---

## ğŸ“– References

1. **GraphCast** â€” Lam et al., DeepMind (2023) â€” [arXiv:2212.12794](https://arxiv.org/abs/2212.12794)
2. **FourCastNet** â€” Pathak et al., NVIDIA (2022) â€” [arXiv:2202.11214](https://arxiv.org/abs/2202.11214)
3. **ClimaX** â€” Nguyen et al., Microsoft (2023) â€” [arXiv:2301.10343](https://arxiv.org/abs/2301.10343)
4. **WeatherBench2** â€” Rasp et al. (2023) â€” [arXiv:2308.15560](https://arxiv.org/abs/2308.15560)
5. **Attention Is All You Need** â€” Vaswani et al. (2017) â€” [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
6. **Vision Transformer (ViT)** â€” Dosovitskiy et al. (2020) â€” [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)

---

## ğŸ‘¤ Author

**Aswani Sahoo**

- GitHub: [@AswaniSahoo](https://github.com/AswaniSahoo)
- LinkedIn: [Aswani Sahoo](https://linkedin.com/in/aswanisahoo)

---

## ğŸ“ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <i>Built with â¤ï¸ as preparation for GSoC 2026 â€” AI for Science</i>
</p>